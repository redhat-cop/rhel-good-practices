{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to RHEL Good Practices","text":"<p>Welcome to our Red Hat Enterprise Linux Good Practices page. Here we aim to collect technical and non-technical good practices for Red Hat Enterprise Linux. </p> <p>We encourage everyone to contribute their best practices. Checkout the contribution guide under \"Getting started\" for more details about that.</p>"},{"location":"#what-is-this-page-not-about","title":"What is this page NOT about","text":"<p>This page gathers various good practices, but it does not:</p> <ul> <li>Replace Red Hat's official documentation.</li> <li>Replace Red Hat support</li> <li>Provide a definite answer for something. We deal with good practices, meaning there may be multiple ways to do something.</li> </ul>"},{"location":"contribute/contributing/","title":"Contributing to <code>redhat-cop/rhel-good-practices</code>","text":"<p>Welcome to the <code>redhat-cop/rhel-good-practices</code> repository! We appreciate your interest in contributing. This guide will walk you through the entire process of contributing to the project, from setting up your environment to submitting your changes.</p>"},{"location":"contribute/contributing/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Setting Up Your Environment</li> <li>Making Changes</li> <li>Creating a Pull Request</li> <li>Code Review Process</li> <li>Additional Resources</li> </ol>"},{"location":"contribute/contributing/#getting-started","title":"Getting Started","text":"<p>Before you start contributing, make sure you have the following: - A GitHub account. - Basic knowledge of Git and GitHub workflows. - Familiarity with the project's purpose and guidelines (check the README).</p>"},{"location":"contribute/contributing/#setting-up-your-environment","title":"Setting Up Your Environment","text":"<ol> <li>Fork the Repository:</li> <li>Go to the repository page.</li> <li>Click the \"Fork\" button at the top right to create a copy of the repository under your GitHub account.    </li> <li> <p>Proceed and create the fork.    </p> </li> <li> <p>Clone Your Fork:</p> </li> <li> <p>Clone your forked repository to your local machine (replace YOUR-USERNAME with your GitHub username):      <pre><code>git clone https://github.com/YOUR-USERNAME/rhel-good-practices.git\ncd rhel-good-practices\n</code></pre></p> </li> <li> <p>Set Upstream Remote:</p> </li> <li> <p>Add the original repository as an upstream remote to sync changes:      <pre><code>git remote add upstream https://github.com/redhat-cop/rhel-good-practices.git\n</code></pre></p> </li> <li> <p>Sync Your Fork:</p> </li> <li>Fetch the latest changes from the upstream repository:      <pre><code>git fetch upstream\n</code></pre></li> <li> <p>Merge the changes into your main branch:      <pre><code>git checkout main\ngit merge upstream/main\n</code></pre></p> </li> <li> <p>Create a New Branch:</p> </li> <li>Create a new branch for your changes:      <pre><code>git checkout -b your-branch-name\n</code></pre></li> </ol>"},{"location":"contribute/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Make Your Changes:</li> <li>Make the necessary changes to the code or documentation.</li> <li> <p>Follow the project's coding standards and guidelines.</p> </li> <li> <p>Commit Your Changes:</p> </li> <li>Stage your changes:      <pre><code>git add .\n</code></pre></li> <li> <p>Commit your changes with a clear and descriptive commit message:      <pre><code>git commit -m \"Your descriptive commit message\"\n</code></pre></p> </li> <li> <p>Push Your Changes:</p> </li> <li>Push your changes to your forked repository:      <pre><code>git push origin your-branch-name\n</code></pre></li> </ol>"},{"location":"contribute/contributing/#creating-a-pull-request","title":"Creating a Pull Request","text":"<ol> <li>Open a Pull Request:</li> <li>Go to your forked repository on GitHub.</li> <li>Click the \"Compare &amp; pull request\" button.     </li> <li>Describe Your Changes:</li> <li>Provide a clear title and description for your pull request.</li> <li> <p>Reference any related issues using <code>#issue-number</code>.</p> </li> <li> <p>Submit the Pull Request:</p> </li> <li>Click \"Create pull request\" to submit your changes for review.</li> </ol>"},{"location":"contribute/contributing/#code-review-process","title":"Code Review Process","text":"<ul> <li>Maintainers will review your pull request and may request changes.</li> <li>If changes are requested, make the necessary updates and push them to your branch. The pull request will automatically update.</li> <li>Once your pull request is approved, it will be merged into the main repository.</li> </ul>"},{"location":"contribute/contributing/#additional-resources","title":"Additional Resources","text":"<ul> <li>GitHub Docs: Contributing to a Project</li> <li>GitHub Flow Guide</li> <li>How to Write a Good Commit Message</li> <li>Git Cheat Sheet</li> </ul> <p>Thank you for contributing to <code>redhat-cop/rhel-good-practices</code>! Your efforts help improve the project for everyone. If you have any questions, feel free to reach out to the maintainers.</p> <p>Happy contributing! \ud83d\ude80</p>"},{"location":"contribute/contribution-guidelines/","title":"Contributing guidelines","text":"<p>The static site generation relies on the open source project mkdocs. mkdocs and the theme we use Material for MkDocs offer plugins and features to enhance markdown rendering leveraging the pymdown-extensions project.</p>"},{"location":"contribute/contribution-guidelines/#editing-an-existing-document","title":"Editing an existing document","text":"<p>To edit an existing piece of documentation, it is sufficient to work directly on the impacted markdown file and follow the contribution quickstart to create and publish the pull request.</p>"},{"location":"contribute/contribution-guidelines/#adding-a-new-document-andor-folder","title":"Adding a new document and/or folder","text":"<p>Adding a new document and/or folder requires two main steps:</p> <ul> <li>Add the folder and/or the document under the docs folder in the root of the repository</li> <li>Add the reference to the new folder in the nav: section of mkdocs.yml file in the root of the repository</li> </ul>"},{"location":"contribute/contribution-guidelines/#example-add-a-document-called-openscap_profilesmd-in-the-security-folder","title":"Example - Add a document called openscap_profiles.md in the security folder.","text":"<ul> <li>Add the openscap_profiles.md file in the docs/security folder</li> <li>Add the new entry in the nav: section of mkdocs.yml file:</li> </ul> <pre><code>nav:\n  - Home: index.md\n  - Getting Started:\n    - Contributing to the project: contributing.md\n  - Security:\n    - Welcome to security: security/welcome.md\n    - Configure OpenScap Profiles: security/openscap_profiles.md # &lt;------ This is the new entry in the Key: Value format\n</code></pre>"},{"location":"contribute/contribution-guidelines/#example-add-a-new-section-called-network-and-add-a-configure_bondmd-file-in-it","title":"Example - Add a new section called Network and add a configure_bond.md file in it","text":"<ul> <li>Create the network folder in the docs main folder.</li> <li>Add the configure_bond.md file in the docs/network folder</li> <li>Add the new entry in the nav: section of mkdocs.yml file:</li> </ul> <pre><code>nav:\n  - Home: index.md\n  - Getting Started:\n    - Contributing to the project: contributing.md\n  - Security:\n    - Welcome to security: security/welcome.md\n  - Network:                                            # &lt;------ Here you define the new section\n    - Configure a bond: network/configure_bond.md       # &lt;------ This is the new entry for the file in the 'Key: Value' format\n</code></pre>"},{"location":"contribute/review-test-guide/","title":"Reviewing and testing the changes","text":"<p>Since documentation can contain snippets and markdown from Material for MkDocs and pymdown-extensions projects, to properly test changes, whether while writing content or reviewing a Pull Request, a Containerfile is provided with the minimum packages to run the serving command for MkDocs.</p> Review Containerfile <pre><code>  FROM registry.access.redhat.com/ubi9/python-312\n  RUN pip3 install mkdocs mkdocs-material mkdocs-macros-plugin\n  ENTRYPOINT mkdocs serve -a 0.0.0.0:8000\n</code></pre>"},{"location":"contribute/review-test-guide/#instructions-to-test-changes","title":"Instructions to test changes","text":""},{"location":"contribute/review-test-guide/#building-the-container-image","title":"Building the container image","text":"<p>While writing content, from the root folder of the repository, simply build the image:</p> <pre><code>podman build -t mkdocs-testing .\n</code></pre>"},{"location":"contribute/review-test-guide/#running-the-container","title":"Running the container","text":"Read here if you are reviewing a Pull Request <p>When reviewing a pull request, you need to create a temporary branch and fetch the content into it. Assuming user kubealex proposed a Pull Request involving the testing branch: <pre><code>git checkout -b kubealex-testing\ngit pull https://github.com/kubealex/rhel-good-practices.git testing\n</code></pre></p> <p>After the image is built, simply run the container mounting the current folder:</p> <pre><code>export HOST_PORT=8000\npodman run -it --user $(id -u) --network podman -p $HOST_PORT:8000 -v ./:/opt/app-root/src:rw,Z mkdocs-testing mkdocs serve -a 0.0.0.0:8000\n</code></pre> <p>Replace the HOST_PORT variable with a free port on the host you are running the container.</p> <p>If everything is working fine, the webserver will be listening on the desired port and reachable at the address http://localhost:8000</p> <p></p>"},{"location":"generic/dotdfolders/","title":"*.d configuration folders","text":"<p>Most system services and applications rely on configuration files for their settings and many packages come with pre-populated files (usually *.conf) with all defaults that can be edited and tailored based on your needs.</p> <p>Editing the main .conf file can introduce challenges including:</p> <ul> <li>The main configuration file could be overwritten during updates/upgrades</li> <li>The main configuration file can change significantly from the default, making it difficult to read and interpret</li> </ul> <p>*.d folders come to the rescue for both scenarios.</p> <p>Many applications include specific folders, with \".d\" in their name (i.e. conf.d, conf.modules.d) that contain:</p> <ul> <li>Configuration snippets -&gt; Chunks of the main config file with only specific overrides</li> <li>Modular configurations -&gt; Dedicated configuration files for specific functionalities</li> <li>How to use them -&gt; Syntax, flags to be enabled/disabled in the application to load files in the directory, etc.</li> </ul> <p>It is a good practice to use them to immediately benefit from:</p> <ul> <li>Better readability</li> <li>Modular approach</li> <li>Persistence across application or OS updates/upgrades</li> </ul>"},{"location":"generic/dotdfolders/#example-httpd","title":"Example - httpd","text":"<p>Let's take httpd as an example.</p> <p>By quickly inspecting its main folder, /etc/httpd/conf/, we can see that it has three configuration folders:</p> <pre><code>ls /etc/httpd/conf*\n/etc/httpd/conf:\nhttpd.conf  magic\n\n/etc/httpd/conf.d:\nautoindex.conf  mod_dnssd.conf  README  userdir.conf  welcome.conf\n\n/etc/httpd/conf.modules.d:\n00-base.conf    00-dav.conf  00-mpm.conf       00-proxy.conf    01-cgi.conf  10-mod_dnssd.conf  README\n00-brotli.conf  00-lua.conf  00-optional.conf  00-systemd.conf  10-h2.conf   10-proxy_h2.conf\n</code></pre> <p>Its main configuration file, httpd.conf stored in /etc/httpd/conf/ folder contains all the default configuration settings for a standard webserver to start and be operative.</p> Extract from httpd.conf file <pre><code>#\n# This is the main Apache HTTP server configuration file.  It contains the\n# configuration directives that give the server its instructions.\n# See &lt;URL:http://httpd.apache.org/docs/2.4/&gt; for detailed information.\n# In particular, see\n# &lt;URL:http://httpd.apache.org/docs/2.4/mod/directives.html&gt;\n# for a discussion of each configuration directive.\n#\n# See the httpd.conf(5) man page for more information on this configuration,\n# and httpd.service(8) on using and configuring the httpd service.\n#\n# Do NOT simply read the instructions in here without understanding\n# what they do.  They're here only as hints or reminders.  If you are unsure\n# consult the online docs. You have been warned.\n#\n# Configuration and logfile names: If the filenames you specify for many\n# of the server's control files begin with \"/\" (or \"drive:/\" for Win32), the\n# server will use that explicit path.  If the filenames do *not* begin\n# with \"/\", the value of ServerRoot is prepended -- so 'log/access_log'\n# with ServerRoot set to '/www' will be interpreted by the\n# server as '/www/log/access_log', where as '/log/access_log' will be\n# interpreted as '/log/access_log'.\n\n#\n# ServerRoot: The top of the directory tree under which the server's\n# configuration, error, and log files are kept.\n#\n# Do not add a slash at the end of the directory path.  If you point\n# ServerRoot at a non-local disk, be sure to specify a local disk on the\n# Mutex directive, if file-based mutexes are used.  If you wish to share the\n# same ServerRoot for multiple httpd daemons, you will need to change at\n# least PidFile.\n#\nServerRoot \"/etc/httpd\"\n\n#\n# Listen: Allows you to bind Apache to specific IP addresses and/or\n# ports, instead of the default. See also the &lt;VirtualHost&gt;\n# directive.\n#\n# Change this to Listen on a specific IP address, but note that if\n# httpd.service is enabled to run at boot time, the address may not be\n# available when the service starts.  See the httpd.service(8) man\n# page for more information.\n#\n#Listen 12.34.56.78:80\nListen 80\n[...]\n# Dynamic Shared Object (DSO) Support\n#\n# To be able to use the functionality of a module which was built as a DSO you\n# have to place corresponding `LoadModule' lines at this location so the\n# directives contained in it are actually available _before_ they are used.\n# Statically compiled modules (those listed by `httpd -l') do not need\n# to be loaded here.\n#\n# Example:\n# LoadModule foo_module modules/mod_foo.so\n#\nInclude conf.modules.d/*.conf\n[...]\n# Supplemental configuration\n#\n# Load config files in the \"/etc/httpd/conf.d\" directory, if any.\nIncludeOptional conf.d/*.conf\n</code></pre> <p>By carefully reviewing the httpd.conf file, we can see that there are two specific directives, \"Include conf.modules.d/*.conf\" and \"IncludeOptional conf.d/*.conf\" that are looking for .conf files in the .d folders.</p> <p>Each of these files contain specific configuration, either for the application or modules (i.e. SSL/TLS), and it is an accurate example of how configuration files can be well organized using .d folders.</p>"},{"location":"generic/dotdfolders/#adding-a-custom-configuration","title":"Adding a custom configuration","text":"<p>An admin could edit the webserver configuration, to create a virtual host for example.net. To do so, it is enough to create a new file, in the conf.d folder, named example-net.conf with the following content:</p> Example custom configuration - example-net.conf <pre><code># This is example config file for virtual host \"example.net\"\n\n# Redirect only vhost. Use permanent url for same resource.\n#&lt;VirtualHost *:80&gt;\n#   ServerAlias example.net\n#   RedirectPermanent / http://www.example.net\n#&lt;/VirtualHost&gt;\n\n&lt;VirtualHost *:80&gt;\n    ServerName www.example.net\n    ServerAdmin webmaster@example.net\n    DocumentRoot /home/services/httpd/vhosts/example.net\n    ErrorLog logs/example.net-error_log\n    TransferLog logs/example.net-access_log\n&lt;/VirtualHost&gt;\n\n#&lt;Directory \"/home/services/httpd/vhosts/example.net\"&gt;\n#  AllowOverride None\n#  Options None\n#  &lt;IfModule mod_authz_host.c&gt;\n#    Require all granted\n#  &lt;/IfModule&gt;\n#&lt;/Directory&gt;\n</code></pre> <p>Any change made to the conf.d and conf.modules.d can be made effective by reloading the httpd service, so to apply the new virtual host:</p> <pre><code>systemctl restart httpd\n</code></pre>"},{"location":"operational-model/advanced/","title":"Operational Model - RHEL - Advanced","text":"<p>If you are wonder what an operational model is, you should start by reading this introduction.</p>"},{"location":"operational-model/advanced/#overview","title":"Overview","text":"<p>Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/advanced/#people","title":"People","text":"<p>Overview of the process section. Zoom into this picture for details.</p> <p></p> <ul> <li>Solution architects - To offload the many discussions on how to do different things with your operating system platform, it makes a lot of sense to have one or several Solution Architects which focuses on helping users implement their systems.  </li> </ul>"},{"location":"operational-model/advanced/#community-of-practice","title":"Community of practice","text":"<p>There are many different names for this, for example Centre of Excellence, DevOps Dojo, or etc. It's simply a community which you build around your platform. Here we describe resources related to people and your community.</p> <ul> <li>Gamification - You have implemented gamification practices which boosts contribution in your community.</li> <li>Contribution princes - You celibrate contribution to the degree that you have a small budget for contribution prices.</li> <li>Chat channels per domain - Your community has grown so large that you now need to split chat channels per technical domain.</li> <li>Re-ocurring meetings per domain - Your community is too large for a single community of practice meeting. You also now have meeting specific for special technical domains such as cloud, specific security.</li> <li>SLA for responses - To ensure that people are not left in a vaccum with their problems, you aim to answer all questions in your chat chanels within a specific time.</li> </ul>"},{"location":"operational-model/advanced/#rhel-platform-processes","title":"RHEL Platform processes","text":"<p>Processes related to the Red Hat Enterprise Linux standard platform itself.</p>"},{"location":"operational-model/advanced/#onboarding-process","title":"Onboarding process","text":"<p>An overview of a common onboarding process can be viewed below. Please note that the graphics describes all three advancement levels of onboarding.</p> <p></p> <p>The steps for the Advanced version of the onboarding follows:</p> <ul> <li>Skill based training (Onboarding for experienced/un-experienced) There is a specific onboarding process for people who are already experienced with modern development practices and Red Hat Enterprise Linux. This allows highly experienced individuals to get started more quickly.</li> </ul>"},{"location":"operational-model/advanced/#development-processes","title":"Development processes","text":"<p>Processes related to the development of your RHEL platform.</p> <ul> <li>Infra as code - As you develop and define your RHEL Platform, you use infrastructure-as-code concepts. This is a requirement to allow for advanced controls of the development and security of the standard.</li> <li>Complete test coverage (Unit, Functional, Integration) - All aspects of your standards is covered by tests. This allows you to quickly pinpoint complex issues as your develop your standard, inroduce new major releases and more.</li> <li>Continous deployment and testing of platform - When you make a change to your standard, it automatically deploys test instances and performs testing on them. This allows you to navigate complicated issues in the standard where something breaks further down the line.</li> <li>Automated RPM builds - You have an automated RPM build system which ties into the automatic and continous deployment and testing of your platform.</li> <li>Advanced supply chain security - The development process of your standard has advanced security compliance such as SLSA Level 3.</li> <li>Ansible, Terraform, etc - To deploy a complete fresh instance of your RHEL platform in an automated fashion you do well in using an automation framework, such as Ansible or Terraform.</li> </ul>"},{"location":"operational-model/advanced/#technology","title":"Technology","text":"<p>Overview of the technology section. Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/advanced/#rhel-architecture","title":"RHEL architecture","text":"<p>Architectural decisions related to Red Hat Enterprise Linux.</p>"},{"location":"operational-model/advanced/#infrastructure-design","title":"Infrastructure design","text":"<p>Infrastructure related decisions.</p> <ul> <li>Self healing system - The installed instances of RHEL will self heal in case of known issues related to Availability or Security are detected. This can be accomplished with things such as Event Driven Ansible which is a part of Ansible Automation Platform, or 3rd party SIEM or monitoring platforms.</li> <li>Ansible Automation Platform - Your requirements for automation and scale of your installed base of RHEL instances means that you would be well off in investing in a general purpose automation platform. The most popular configuration management tool is Ansible, and the premier central automation platform is called Ansible Automation Platform.. This allows your to streamline development of your platform, moving automation previously defined elsewhere, as Ansible automation. It will also decrease overall technical risk for your platform and allow you to focus your time on things which matters more.</li> <li>Military Grade Security - Security standards such as CIS Level 2 are not good enough. Creating automatic hardening which can withstand persitent threat groups requires the most strictest security hardening, including SELinux MLS mode, Integrity Measurement Architecture, etc, and agressive automatic security responses.</li> <li>Advanced image build system - When you need to support a broader range of RHEL installation images, you need to invest more in your image build system. It needs to be able to continously generate images, also integrated to your automatic RPM build system - if you have one.</li> <li>Multiple security baselines - To support more diverse security requirements, you may have to add multiple security baselines to provide a more broad spectrum of compliance. This is challenging as it may spawn many more images or kickstarts which you need to create and maintain. As an example: If you maintain specific installation images for two different technical platforms, let's say: Public Cloud A, and Baremetal, you may now get Public Cloud A - Compliance A, Public Cloud A - Compliance B images, and so on.</li> </ul>"},{"location":"operational-model/advanced/#rhel-design","title":"RHEL design","text":"<p>Design of your Red Hat Enterprise Linux standard.</p> <ul> <li>Application platform specific post configuration - Your platform automatically applies post configuration specific to different application platforms such as different databases, application servers, etc.</li> <li>Fully unattended installations - When a new instance of your standard starts up, nothing more is needed to be done as it relates to your platform. That includes configuring all related systems, such as CMDB systems, change management systems, and so on.</li> <li>Multi-platform support - You support multiple hardware-, virtualization- and cloud platforms.</li> <li>RHEL Image mode - Your RHEL platform also supports RHEL Image mode.</li> </ul>"},{"location":"operational-model/advanced/#development-tool-chain","title":"Development tool chain","text":"<p>Features related to the development tool chain.</p> <ul> <li>Complete test coverage (Unit, Functional, Integration) - You need tools which supports the complete testing of your platform. That can be done using the automation, infra as code and CI/CD features of your platform. It may also be accomplished with 3rd party tools test platforms.</li> <li>RPM Build system You need a system which can automatically build RPMs. Either a custom implementation using rpm-build tools, mock, or a more proper platform such as Koji.</li> <li>Advanced supply chain security - You need to select tools such can support advanced supply chain security standards such as SLSA.</li> <li>Ansible, Terraform, etc - You need more advanced automation capabilities to be able to automatically deploy your standard, such as Ansible or Terraform.</li> </ul>"},{"location":"operational-model/intro/","title":"What is an operational model?","text":"<p>Hello and welcome. This is a good practice based operational model for Red Hat Enterprise Linux. This model is for people who wants inspiration for how you can run and manage a Linux standard platform based on Red Hat Enterprise Linux.</p> <p>This model covers good practices and should not be read as a list of things you always need to do. Different organizations are different.</p> <p>The model, which covers people, process, technology, service management and strategy is divided between three different advancement levels (1-3). Access these different levels via the menu on the top of this page.</p> <p>It\u2019s likely that your best fit is a mix of the different levels (MVP, 2.0, Advanced).</p> <p>An overview of the complete operational model, with all sub-sections can be viewed below. Zoom into the picture for details:</p> <p></p> <ol> <li>Minimal Viable Product (MVP)</li> </ol> <p>The MVP version of a platform definition is a common good practice starting point, which we can build on. Due to the limitation in regards to automation, this works for a limited install base of servers. We are weighing things such as ease-of-adoption and standardization against effort to implement. Instead of creating a so-called \u201cbig bang release\u201d, which requires significant investments in regards to effort and people, we ensure we have just enough platform features to provide our platform for some first real life use-cases.</p> <p>Read more here.</p> <ol> <li>Two point zero (2.0)</li> </ol> <p>Once there has been some real degree of success and adoption of the platform, it\u2019s time to fine tune the definition for it to be able to meet the requirements of the future. Focus is on more advanced platform and adoption related capabilities. We spend more time standardizing things such as how we develop, install and configuration the platform, an investment which is required as the burden of compliance otherwise threatens to grind our organization to a halt. The features we now build reduces the effort regarding maintenance and compliance, and allows us to maintain a much larger install base of Linux installation.</p> <p>Read more here.</p> <ol> <li>Advanced</li> </ol> <p>A lot of focus now goes into making sure the platform install base can grow to significant scales and make more signficant business impact. Our platform is one of the key technologies used in our organization and needs to be able to cater for much stricter requirements. At the same time we need to spend increased effort on the community we have built around our Linux platform. Some final technical capabilities are put in place to ensure we can meet any type of business requirements.</p> <p>Read more here.</p>"},{"location":"operational-model/mvp/","title":"Operational Model - RHEL - Minimal Viable Product","text":"<p>If you are wonder what an operational model is, you should start by reading this introduction.</p>"},{"location":"operational-model/mvp/#people","title":"People","text":"<p>Overview of the people section. Zoom into this picture for details.</p> <p></p> <ul> <li>It is common that the people who operate and maintain the platform has other jobs and do not work dedicated on the platform. Or it could simply just be a group of people with system admin type jobs. Even though we do not yet require a dedicated platform owner or platform engineering team, we should already start thinking about that.</li> </ul>"},{"location":"operational-model/mvp/#process","title":"Process","text":"<p>Overview of the process section. Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/mvp/#rhel-platform-processes","title":"RHEL Platform processes","text":"<p>Processes related to the Red Hat Enterprise Linux standard platform itself.</p> <ul> <li>Incident management - We need a standard process for managing technical incidents for the platform. Who should people contact if something goes wrong with their Linux server? And what will we do if something breaks?</li> <li>Life Cycle Management - It is important that we communicate when we plan to update our platform. This in a form of a life-cycle management plan. At this point, if you do not know much about this, you can copy the Red Hat Enterprise Linux life-cycle management plan, which you find here: Red Hat Enterprise Linux Life Cycle</li> <li>Backup and recovery - We need to have a standard process for doing backups and recovering from those backups. Life cycle management and incident management depends on this.</li> <li>Patch Management - We need a process for how to patch our RHEL installations. This can include moving between minor releases of RHEL, but not major releases as that is a special workflow. In part this process needs to cover how you identify systems which needs to be patched and then how the system is patched. Initially, this will be a manual process, later on, this becomes an automated process.</li> <li>Capacity management - How do we monitor the utilization of an installed RHEL server, and act as we need to expand the capacity? Put in place a process to deal with this. It's normal that the process at this point is manual.</li> <li>Subscription management - How do you keep track of your Red Hat Enterprise Linux subscription usage? If your systems can conntact to Red Hat's services, then you will be able to manage your subscriptions via Red Hat's online Subscription Management page. You should create a process for keeping track of utilization, so that you can order more subscriptions, before you actually run out. You can reach out to your Red Hat account team to get help with designing such a process.</li> </ul>"},{"location":"operational-model/mvp/#onboarding-process","title":"Onboarding process","text":"<ul> <li>Manual onboarding - The onboarding process is commonly not automated. When new people or teams get onboarded, the people who works with the platform are responsible for this process. This process is very simple in its nature, lacking assessment of who and what is onboarded and training. Because of this - what people end up doing with their systems, and how they will manage them, is very different. If you need to onboard more than a few teams, you should have more advanced.</li> </ul> <p>An overview of a common onboarding process can be viewed below. Please note that the graphics describes all three advancement levels of onboarding.</p> <p></p> <p>The steps for the MVP version of the onboarding process follows:</p> <ul> <li>Users provided access to landing page - The landing page is referenced and described in the overall operational model. It is a page where users can go and get information such as platform documentation, descriptions of process (such as the onboarding process itself) and documentation and training materials/links.</li> <li>LDAP groups created - If your organization is using an LDAP solution for identity, create some standard LDAP groups which you can connect to the your RHEL installations. It's good if you create some type of standard naming here, such as: rhel-(system-name)-(team name)-admin, rhel-(system-name)-(team name)-user, etc. The team in question can then own these groups and do user management themselves.</li> <li>Users added to group - Users are added to the groups in question. This may be done by the team who are being onboarded.</li> <li>RBAC configuration created on servers - You now put in place the required RBAC related configuration on your server, as it relates to things such as sudoers, ssh, pam, sssd, etc. </li> </ul>"},{"location":"operational-model/mvp/#development-processes","title":"Development processes","text":"<ul> <li>There are no standardized processes yet, related to how we are developing our RHEL platform. That also puts some limitations for how much we can scale at this point, as that would risk racking up a lot of technical debt. As an example, if we have to perform a lot of changes to our RHEL standard, that will take a lot of time and effort, while it's likely that those changes will not be applied consistently across our installed base.</li> </ul>"},{"location":"operational-model/mvp/#technology","title":"Technology","text":"<p>Overview of the technology section. Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/mvp/#rhel-architecture","title":"RHEL architecture","text":"<p>Architectural decisions related to Red Hat Enterprise Linux.</p>"},{"location":"operational-model/mvp/#infrastructure-design","title":"Infrastructure design","text":"<p>Infrastructure related decisions.</p> <ul> <li>Availability - We need to design our platform so that it can meet our requirements for availability. That may mean that we need to support to run on a specific server or virtualization platform, or support specific storage configurations related to SAN.</li> <li>Security - We need to design our platform so that it also meets requirements for security, that includes hardening practices for RHEL, supporting network segmentation, 3rd party security tools, etc.</li> <li>Monitoring - To ensure availability and security, some type of monitoring needs to be put in place. How do you know if utilization of CPU, memory or storage goes through the roof? How do you know that your backup client or security related services are running? Monitoring.</li> <li>Repository management - If your systems can connect to Red Hat's YUM repositories, then this item is resolved. Please note that when you do update your systems, they will be updated to whatever latest versions are available. If you want control over what version of software gets installed on your RHEL servers, or if your systems cannot be connected to Red Hat provided YUM repositories, then you need to decide how you will manage the RHEL yum repositories yourself. Via some 3rd party system or by simply synchronizing related repositories to some local NFS or HTTP server?</li> <li>Subscription management - If your systems can conntact to Red Hat's services, then you will be able to manage your subscriptions via Red Hat's online Subscription Management page. If this is not the case, you should invent some custom temporary mean to count how many subscriptions are currently used, perhaps by development a document for this purpose.</li> </ul>"},{"location":"operational-model/mvp/#rhel-design","title":"RHEL design","text":"<p>Design of your Red Hat Enterprise Linux standard.</p> <ul> <li>Package baseline - You have to decide what packages should be a part of your RHEL standard installation. Red Hat has some standard RHEL package definitions such as core, base, which can inspire you. Also, if you have a look at a standard RHEL KVM or Cloud images, that can also be used for inspiration.</li> <li>User and identity management - How will you do users management in your RHEL standard? Will you allow the use of local users? Will you require the use of some LDAP service for identity and authentication? Will you mandate the use of service accounts for automation? You better decide these things now, before you have hundreds of servers.</li> <li>RBAC - This relates to configuring who gets access to do what. Will everyone have root access? Or will only some people have this? This is where you decide on any separation of duty enforcements and configure related services such as sssd, sshd, sudoers, pam, etc.</li> <li>Installation - Decide on how to install Red Hat Enterprise Linux. Starting off with your RHEL Linux standard platform, it is not unusual that people manually installs RHEL. With that said, nowdays it's also common that you start off with some automated installation, using standard RHEL images you download and built in virtualization or cloud features such as cloud init.</li> <li>Manual patch management - In the beginning of your RHEL journey, it's not uncommon that patch management is a manual process. You still need to decide on how to do patching though. This includes the process of making patches available to your systems, ensuring backups are in place, installing updates, testing so that nothing broke, etc. If this process is not automated it soon becomes very painful to scale the numbers of your installed based of RHEL. So prioritize automating this later on.</li> </ul>"},{"location":"operational-model/mvp/#development-tool-chain","title":"Development tool chain","text":"<p>Features related to the development tool chain.</p> <ul> <li>Git organization - How do you organize things such as related automation, in your version control system? Do you gather all repositories in the same group, or something else?</li> <li>Git repository structure - If you end up maintaining a lot of different respositories, it may be a good idea to decide how these repositories should work. Will you commit everything straight to the main branch, or will you have a more refined git branching strategy?</li> </ul>"},{"location":"operational-model/mvp/#onboarding","title":"Onboarding","text":"<p>Technology implementations related to onboarding.</p> <ul> <li>Landing page - When someone gets onboarded to your platform. They need a place to start their onboarding process. This is typically a wiki page or webpage maintained by the core platform team.</li> <li>Platform documentation - A common part of a technical platform implementation, you have documentation which decribes to other people how the platform works.</li> <li>Link collection - A good way to help people who get onboarded is by maintaining a list of useful links.</li> </ul>"},{"location":"operational-model/mvp/#platform-management","title":"Platform management","text":"<p>Overview of the platform management section. Zoom into this picture for details.</p> <p></p> <p>Administrative features which helps with the development or maintenence of the platform.</p> <ul> <li>Service Level Agreement - Select what different SLAs your platform should support.</li> <li>Operational Level Agreement - Select what different OLAs your platform should support (if you use OLAs).</li> <li>Feature planning - Make sure that as you plan and prioritize the different capabilities of your platform. A good start is to benchmark yourself against the model and then prioritize the different features across the MVP, 2.0 and Advanced levels. Except for creating more structure for the platform building process - this allows you to communicate detailed information about future feature, to the users of the platform.</li> </ul>"},{"location":"operational-model/twozero/","title":"Operational Model - RHEL - Minimal Viable Product","text":"<p>If you are wonder what an operational model is, you should start by reading this introduction.</p>"},{"location":"operational-model/twozero/#people","title":"People","text":"<p>Overview of the people section. Zoom into this picture for details.</p> <p></p> <ul> <li>It is common that the people who operate and maintain the platform has other jobs and do not work dedicated on the platform. Even though we do not yet require a dedicated platform owner or platform engineering team, we should already start thinking about that.</li> </ul>"},{"location":"operational-model/twozero/#community-of-practice","title":"Community of practice","text":"<p>There are many different names for this, for example Centre of Excellence, DevOps Dojo, or etc. It's simply a community which you build around your platform. Here we describe resources related to people and your community.</p> <ul> <li>Community of practice core team - This is commonly made up by the product owner and platform engineers, but as you have created a community of practice (devops dojo, center of excellence, etc), you may want to invite some people from your community to take on leading roles.</li> <li>Community of practice members - The community consists of people who are interested in automation and your platform. You gotta catch em all to get get a strong and growing community, but it's good to start small, as there is a lot to learn here.</li> <li>Chat channel - Community of pratice members needs a place where they can talk to each other.</li> <li>Re-ocurring meeting - Schedule a meeting every other week or weekly, where people can share and interact with each other.</li> </ul>"},{"location":"operational-model/twozero/#process","title":"Process","text":"<p>Overview of the process section. Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/twozero/#rhel-platform-processes","title":"RHEL Platform processes","text":"<p>Processes related to the Red Hat Enterprise Linux standard platform itself.</p> <ul> <li>Disaster recovery - We ensure we have a functioning process for Disaster Recovery. When sigificant parts of the organization depends on our platform, we need this. This is a fairly complicated process it relates to rebuilding your complete RHEL standard platform, including potential systems you use to create it with, depending on what disaster scenario you are mitigating. </li> <li>Upgrading - It's now time for you to get a dedicated process for how to manage upgrades between major releases of Red Hat Enterprise Linux. There are many options available, either by using LEAPP or by having an approach where you install new servers which you then migrate to.</li> <li>Automated onboarding - The process for onboarding should be an automated one, this allows us to quickly onboard new teams and reduce shadow IT created by people tired of waiting for things.</li> <li>Assessment of teams and workloads - When you onboard new teams it's important that you do some type of assessment of what type of workload will run on their servers. Workloads are an important source of requirements for things such as availability, capacity and security. You can start small and simply just interview new teams, or have them submit some standard information as a part of the onboarding process.</li> <li>Training - You now do standard training for people who you onboard. What is that standard training? You decide.</li> </ul>"},{"location":"operational-model/twozero/#onboarding-process","title":"Onboarding process","text":"<ul> <li>Automatic onboarding - n order to speed up the onboarding process, it is key that you automate as much as possible. This will also decrease the risk that something is not properly configured, such as RBAC configuration in AAP. It's important that people do not have to wait too long, as that risks create shadow IT, where people simply install Linux themselves.</li> </ul> <p>An overview of a common onboarding process can be viewed below. Please note that the graphics describes all three advancement levels of onboarding.</p> <p></p> <p>The steps for the 2.0 version of the onboarding process follows: * Automated onboarding - Configuration required to provide access to a new team is performed automatically. This includes creation of LDAP groups, adding users and configuring systems to provide access to the newly onboarded team. * Initial assessment of workload - As new team are onboarded you need to do assessments of the team and their workloads to be able to provide a good service and speed up adoption. * Requirements assessment - Ask the team some simple questions related to what they will actually run on their Linux instances. Depending on what they answer, it may be good to add new features to your platform, to support their workload. As an example, if the team requires PCI DSS compliance, you may add PCI DSS hardening in your Red Hat Enterprise Linux standard, and get Red Hat Satellite to get simplified compliance reporting. * Training assessment - Just providing someone a tool, without any instructions about how to use it will hurt standardization and securiry and can create all sorts of long term challenges. Assess what type of training the new team needs. * DevOps training required - It is common that newly onboarded teams needs training related to modern practices to manage Linux, this may include things such as: version control, testing, CI/CD and such. This will be key for them to understand. If you fail to catch this type of training requirement that will negatively impact overall standardization for your installed base. * Linux training required - Does the team getting onboarded need training related to how Red Hat Enterprise Linux works? It's good if you have some hands-on training regarding that, as an operating system is a complicated thing which no person can be expected to understand without training. * Training - This is the delivery of the training. If you cannot deliver this yourself, many companies, including Red Hat - has training available.</p>"},{"location":"operational-model/twozero/#development-processes","title":"Development processes","text":"<p>Processes related to the development of your RHEL platform.</p> <ul> <li>Documentation - You have requirements related to what you should document regarding your platform features.</li> <li>External contributions - How do people outside of the RHEL platform team contribute features to the standard? That should be allowed as it creates cross team collaboration and puts the users of RHEL in the driving seat for the standard they use.</li> <li>Testing - When you have developed a new feature in your RHEL standard, how do you test it? Initially this may be a semi-manual process, where you do an installation and see if it was successful. As you move forward, this will have to be automated.</li> <li>CI/CD - In order to automate your development processes, you need CI/CD. But how do you work with CI/CD? That is what this process describes.</li> <li>RHEL best practices - It's common that you have a standard for how to write a programming language, based on best practices. In the same way, there are best practices for how to do things in Red Hat Enterprise Linux. Sit down and consider what design principles and best practices are and then document it. This will help people who contributes to your standard as well.</li> </ul>"},{"location":"operational-model/twozero/#technology","title":"Technology","text":"<p>Overview of the technology section. Zoom into this picture for details.</p> <p></p>"},{"location":"operational-model/twozero/#rhel-architecture","title":"RHEL architecture","text":"<p>Architectural decisions related to Red Hat Enterprise Linux.</p>"},{"location":"operational-model/twozero/#infrastructure-design","title":"Infrastructure design","text":"<p>Infrastructure related decisions.</p> <ul> <li>Capacity mangement - What happens if a system runs out of resources such as CPU, memory, disk or even network bandwidth? Design your standard so that you can easily extend these resources. This commonly requires some cross-team collaboration with teams who manages these resources.</li> <li>Insights management - Red Hat Insights is included in each RHEL subscription. It provides you with a host of information. About the patch state of systems, known vulnerabilities. Also, are you tired of working reactively? It can also provide you advise on how to work proactively to ensure that issues which may become problems - does not.</li> <li>Image build system - If you install RHEL by creating a system from an image, such as virtual machine template or etc, then you will need a system dedicated to building such images. Also, if you are to use Red Hat Enterprise Linux Image Mode (RHEL 9.6+, 10+), you will need a system to build those images. Learn more about that here. </li> <li>Red Hat Satellite - It's not uncommon that this is something you get immediately for your first release, and there are some good reasons for that. Red Hat Satellite helps with a host of challenges related to subscription management, repository management, automation and compliance reporting. Learn more about Red Hat Satellite, here.</li> <li>Security compliance (PCI DSS, CIS, etc) - Sooner or later, most organizations needs to comply with some specific security standard. For you, that likely means you need to cater for that, including providing hardening and compliance reporting for your RHEL platform. But how? You can for example use Red Hat Satellite and Ansible Automation Platform for this.</li> </ul>"},{"location":"operational-model/twozero/#rhel-design","title":"RHEL design","text":"<p>Design of your Red Hat Enterprise Linux standard.</p> <ul> <li>Security baseline - Before you have hundreds of RHEL servers installed, you should create a security baseline which hardens the security posture of you platform. This may also include specific hardening profiles you create for specific security compliance with standards such as PCI DSS, CIS, HIPAA, etc.</li> <li>Automated post config - All the things which you have done manually after installing RHEL, are now things which you need to automate. If you do not do this, you will not be able to scale your install base without some very significant issues. This is also how you can make the consumption of your standard simpler than running off to some cloud provider, by applying all needed configuration for a specific system - automatically. This is also key as it relates to security compliance and hardening - that it is done immediately as the system is created.</li> <li>Kickstart - If you are installing a lot of systems on baremetal servers, it's time for you to consider automating that. If you cannot use image based installations, you will for Red Hat Enterprise Linux use kickstart to accomplish this. Red Hat Satellite supports kickstart installations, if you do not have a system for this already.</li> <li>Image building - If you are delivering your standard in the form of virtual machine or container images, then you to design your standard for these purposes.</li> <li>Automated patch management - In order to be able to scale your install base to serveral hundreds of systems - while you at the same time patch these systems on a regular basis - you need automated patch management. Systems such as Red Hat Satellite or Ansible Automation Platform can help you accomplish this.</li> </ul>"},{"location":"operational-model/twozero/#development-tool-chain","title":"Development tool chain","text":"<p>Features related to the development tool chain.</p> <ul> <li>IDE - This is not a hard requirement, but if you standardize what Integrated Development Environment (VSCode, vim, etc), then that may help people later on as you perform more and more development type tasks in the maintenence of your platform.</li> <li>CI/CD - In order to automate your development processes, you need CI/CD. But what CI/CD platform should you use for this? Decide!</li> <li>Testing - Decide how you will technically perform tests of you RHEL standard platform. Will you use some automation which installs your standard on a loop and does some integration tests after each loop? Do you have some standard test platform which should be used? Being able to prove that your standard is intact as you release new version of it is vital for almost any advanced features, so don't skip this.</li> <li>Building - As it relates to shipping software to RHEL, if you are not using containers, you should be using RPMs. The challenge is that not many people know how to build RPMs, so it may be a good thing for you to provide a system where users of the platform can more easily build their RPMs.</li> <li>Security gates - Related to the creation of your standard, it's increasingy a common requirement that you have secured the development process. This is done by introducing security gates which for example automatically signs or cryptographically validates resources built or used.</li> <li>Deployment For you to be able to automatically test your standard, it needs to be deployed automatically. But how? Perhaps using some automation tool, such as Ansible or Terraform - which are triggered from your RHEL Platform development CI/CD pipeline.</li> </ul>"},{"location":"operational-model/twozero/#onboarding","title":"Onboarding","text":"<p>Technology implementations related to onboarding.</p> <ul> <li>Landing page - When someone gets onboarded to your platform. They need a place to start their onboarding process. This is typically a wiki page or webpage maintained by the core platform team.</li> <li>Platform documentation - A common part of a technical platform implementation, you have documentation which decribes to other people how the platform works.</li> <li>Link collection - A good way to help people who get onboarded is by maintaining a list of useful links.</li> </ul>"},{"location":"operational-model/twozero/#platform-management","title":"Platform management","text":"<p>Overview of the platform management section. Zoom into this picture for details.</p> <p></p> <p>Administrative features which helps with the development or maintenence of the platform.</p> <ul> <li>RHEL Success Plan - There is a basic success plan in place for the platform, outlining what KPIs we want to impact. We are often lacking performance monitoring for these KPIs.</li> <li>KPIs (Master doc) - You have a document describing KPIs which we monitor, if they are not a part of the overall AAP success plan. </li> </ul>"},{"location":"security/certificate-management/","title":"Basic certificate management","text":"<p>Have you ever had to create custom java trust store or been greeted by an error message like <code>x509: certificate signed by unknown authority</code>?  Then this is section is for you.</p>"},{"location":"security/certificate-management/#background","title":"Background","text":"<p>Many times enterprises may choose to have an internal PKI and Certificate Authority not trusted by anyone outside the organization. The drawback of this is that the CA's certificate needs to be distributed on all devices that need to trust the corporate CA.</p> <p>In the past this has meant that developers have been maintaining custom trust stores for their application runtime. Most commonly seen with Java.</p> <p>As most runtimes now use the operating systems trust store by default, adding the CA trust to the system trust instead removes the need for the application developer to bring along their own trust store and maintaining it separately when this instead can be part of the corporate Standard Operating Environment (SOE).</p>"},{"location":"security/certificate-management/#recommended-practice","title":"Recommended practice","text":"<p>As these certificates usually does not change very frequently the recommended practice on how to distribute these certificates is by packaging them up as an RPM and install them as part of the SOE.</p>"},{"location":"security/certificate-management/#references","title":"References","text":"<p>Additional details about the RHEL system trust store and how to add additional certificates to it is available in the product documentation.</p>"},{"location":"security/selinux/","title":"SELinux Documentation for RHEL","text":"<p>SELinux (Security-Enhanced Linux) is a mandatory access control (MAC) system integrated into RHEL. It provides fine-grained security policies for processes, users, and files, beyond standard Linux permissions.</p>"},{"location":"security/selinux/#1-selinux-modes","title":"1. SELinux Modes","text":"<p>SELinux can operate in three modes:</p> Mode Description Enforcing SELinux enforces policies. Access denials are logged and blocked. Permissive SELinux logs policy violations but does not block access. Useful for testing. Disabled SELinux is completely turned off. <p>Check current mode:</p> <pre><code>getenforce\n</code></pre> <p>Check detailed SELinux status:</p> <pre><code>sestatus\n</code></pre> <p>Change mode temporarily (no reboot needed):</p> <pre><code>sudo setenforce 0   # Permissive\nsudo setenforce 1   # Enforcing\n</code></pre> <p>Change mode permanently: Edit <code>/etc/selinux/config</code>:</p> <pre><code>SELINUX=enforcing   # Options: enforcing, permissive, disabled\n</code></pre>"},{"location":"security/selinux/#2-selinux-contexts","title":"2. SELinux Contexts","text":"<p>Every file, directory, and process has an SELinux context, defined as:</p> <pre><code>user:role:type:level\n</code></pre> <ul> <li>user: SELinux user identity</li> <li>role: Role associated with user</li> <li>type (domain): Primary control element; most access decisions use type</li> <li>level: MLS/MCS security level (mostly used in multi-level security systems)</li> </ul> <p>View file context:</p> <pre><code>ls -Z /path/to/file\n</code></pre> <p>View process context:</p> <pre><code>ps -eZ | grep &lt;process&gt;\n</code></pre>"},{"location":"security/selinux/#3-common-selinux-file-types","title":"3. Common SELinux File Types","text":"<p>SELinux relies heavily on types to control access. Some common types:</p> Type Description <code>httpd_sys_content_t</code> Web server static content <code>httpd_sys_rw_content_t</code> Web server writable content <code>var_log_t</code> Log files <code>tmp_t</code> Temporary files <code>user_home_t</code> User home directories <p>Change a file\u2019s context:</p> <pre><code>sudo chcon -t &lt;type&gt; /path/to/file\n</code></pre> <p>Restore default context:</p> <p><pre><code>sudo restorecon -Rv /path/to/file_or_dir\n</code></pre> Set context permanently using <code>semanage fcontext</code>:</p> <pre><code>sudo semanage fcontext -a -t &lt;type&gt; '/path/to/file_or_dir(/.*)?'\nsudo restorecon -Rv /path/to/file_or_dir\n</code></pre> <ul> <li><code>-a</code>: Add a new file context mapping</li> <li><code>-t &lt;type&gt;</code>: Specify the SELinux type</li> <li><code>'/path/to/file_or_dir(/.*)?'</code>: Regular expression for the path and its contents</li> </ul> <p>View all custom file contexts:</p>"},{"location":"security/selinux/#semanage-fcontext-l","title":"<pre><code>semanage fcontext -l\n</code></pre>","text":""},{"location":"security/selinux/#4-booleans","title":"4. Booleans","text":"<p>SELinux has booleans to allow specific actions without changing policies.</p> <p>List all SELinux booleans:</p> <pre><code>getsebool -a\n</code></pre> <p>Enable or disable a boolean temporarily:</p> <pre><code>sudo setsebool httpd_can_network_connect on\n</code></pre> <p>Make boolean change permanent:</p> <pre><code>sudo setsebool -P httpd_can_network_connect on\n</code></pre>"},{"location":"security/selinux/#5-troubleshooting","title":"5. Troubleshooting","text":""},{"location":"security/selinux/#a-audit-logs","title":"a) Audit Logs","text":"<p>SELinux denials are logged in <code>/var/log/audit/audit.log</code>. Use <code>ausearch</code> or <code>audit2allow</code> to analyze:</p> <pre><code>sudo ausearch -m avc -ts today\nsudo ausearch -m avc -ts today | audit2allow -m mymodule\n</code></pre>"},{"location":"security/selinux/#b-troubleshoot-denials","title":"b) Troubleshoot Denials","text":"<ol> <li>Temporarily set permissive mode to test:</li> </ol> <pre><code>sudo setenforce 0\n</code></pre> <ol> <li>Review the denial logs.</li> <li>Create a policy module if needed:</li> </ol> <pre><code>sudo audit2allow -M mymodule &lt; /var/log/audit/audit.log\nsudo semodule -i mymodule.pp\n</code></pre>"},{"location":"security/selinux/#6-best-practices","title":"6. Best Practices","text":"<ul> <li>Keep SELinux in Enforcing mode on production servers.</li> <li>Use <code>restorecon</code> rather than <code>chcon</code> for persistent changes.</li> <li>Use <code>semanage fcontext</code> for permanent file context changes.</li> <li>Use booleans instead of modifying policies whenever possible.</li> <li>Regularly monitor <code>/var/log/audit/audit.log</code> for unexpected denials.</li> <li>Test new applications in Permissive mode before moving to Enforcing.</li> </ul>"},{"location":"security/selinux/#7-useful-selinux-commands","title":"7. Useful SELinux Commands","text":"Command Description <code>getenforce</code> Show current mode (Enforcing/Permissive/Disabled) <code>sestatus</code> Detailed SELinux status <code>setenforce 0</code> Temporarily set permissive/enforcing mode <code>ls -Z</code> Show file contexts <code>ps -eZ</code> Show process contexts <code>restorecon -Rv &lt;path&gt;</code> Restore default context <code>chcon -t &lt;type&gt; &lt;file&gt;</code> Change file context temporarily <code>semanage fcontext -a -t &lt;type&gt; &lt;file&gt;</code> Add a permanent file context mapping <code>semanage fcontext -l</code> List all custom file contexts <code>getsebool -a</code> List all SELinux booleans <code>setsebool -P &lt;boolean&gt; on</code> Permanently enable/disable a boolean <code>audit2allow</code> Generate SELinux policy from audit logs"},{"location":"system-administration/adding-storage/","title":"When you have added new persistent storage to a system","text":"<p>When you have added new persistent storage definitions to <code>/etc/fstab</code> on your system. A good practice is to see if those definitions work, before you reboot your system. Doing so prevents getting stuck in a situation where your system does not boot up, due to faulty /etc/fstab definitions.</p>"},{"location":"system-administration/adding-storage/#practice","title":"Practice","text":"<p>After having added a new definition to /etc/fstab test it by running:</p> <pre><code># mount -a\n</code></pre> <p>If your definitions are without issues, the command with exit without any output, as such:</p> <pre><code># mount -a\n#\n</code></pre> <p>If there are issues which the system will encounter doing boot, you will find those issues before rebooting it, allowing you to fix the issues before the system is rebooted.</p>"},{"location":"system-administration/boot-time/","title":"Troubleshooting long boot time","text":"<p>Here is a tip for when you have a (RHEL 8 or later) system which takes a surprising amount of time to boot. Boot time for a system can be a good general indicator that something may be wrong with the system, and may be something which you want to monitor in general of for specific services.</p>"},{"location":"system-administration/boot-time/#practice","title":"Practice","text":"<p>Use systemd-analyze to get detailed timing information from the last boot.</p> <p>For example:</p> <ul> <li>Get an overview of how long it took for a system to boot, also time split between kernel, initrd and userspace: <pre><code># systemd-analyze\n</code></pre></li> </ul> <p>Example output: <pre><code># systemd-analyze\nStartup finished in 817ms (kernel) + 1.943s (initrd) + 2.978s (userspace) = 5.738s \nmulti-user.target reached after 2.421s in userspace.\n</code></pre></p> <ul> <li>Get a sorted list of what contributed most to a long boot process: <pre><code># systemd-analyze blame\n</code></pre></li> </ul> <p>Example output: <pre><code># systemd-analyze blame\n\n67.100s badly-written.service\n2.217s dev-ttyS2.device\n2.217s sys-devices-platform-serial8250-serial8250:0-serial8250:0.2-tty-ttyS2.device\n2.210s sys-devices-platform-serial8250-serial8250:0-serial8250:0.1-tty-ttyS1.device\n2.210s dev-ttyS1.device\n2.207s dev-ttyS3.device\n2.207s sys-devices-platform-serial8250-serial8250:0-serial8250:0.3-tty-ttyS3.device\n2.205s dev-ttyS0.device\n[ ... removed ... ]\n  14ms dracut-mount.service\n  12ms modprobe@fuse.service\n  12ms systemd-user-sessions.service\n  12ms dracut-pre-mount.service\n  12ms systemd-fsck-root.service\n  11ms initrd-udevadm-cleanup-db.service\n   8ms sys-fs-fuse-connections.mount\n   4ms modprobe@configfs.service\n</code></pre></p> <p>For more systemd-analyze tricks, have a look at the SYSTEMD-ANALYZE(1) man page on your system.</p>"},{"location":"system-administration/helper-scripts/","title":"Introducing helper scripts to make your life easier","text":"<p>Working in the terminal often means typing long commands with many options and parameters. While powerful, this can become repetitive and error-prone, that\u2019s where helper scripts come in.</p> <p>A helper script can be even a very short Bash script that wraps long commands or sets of commands, with relevant tags an parameters, and make it available in a more meaningful and short name.</p> <p>A perfect example is an <code>rsync</code> backup command, that can be easily trimmed down into a simple, reusable script.</p>"},{"location":"system-administration/helper-scripts/#1-the-problem","title":"1. The problem","text":"<p>A typical <code>rsync</code> command for backups might look like this:</p> <pre><code>rsync -avh --progress --delete --exclude='.cache' --exclude='*.tmp' /home/user/ /mnt/backup/home_user/\n</code></pre> <p>While this is something we could already be used to, have it in the history, it could still be handy to have something simpler and immediate in our pocket.</p> <p>We can then start drafting our first helper script.</p>"},{"location":"system-administration/helper-scripts/#2-creating-a-helper-script","title":"2. Creating a helper script","text":"<p>We will start with a very basic version of the script, very useful when we have a fixed or non-frequent changed set of parameters.</p> <p>Start by opening a new file:</p> <pre><code>vi ~/backup\n</code></pre> <p>Add the following script in it:</p> <pre><code>#!/bin/bash\n# backup.sh - helper script for rsync backups\n\n# Parameter for the source folder\nSOURCE=\"/home/user/\"\n# Parameter for the destination folder\nDEST=\"/mnt/backup/home_user/\"\n\n# Run rsync with the following options:\n# -a : archive mode (preserves permissions, symlinks, etc.)\n# -v : verbose\n# -h : human-readable numbers\n# --progress : show progress during transfer\n# --delete : remove files on the destination that no longer exist in the source\n# --exclude : skip certain files/folders\n\nrsync -avh --progress --delete \\\n  --exclude='.cache' \\\n  --exclude='*.tmp' \\\n  \"$SOURCE\" \"$DEST\"\n</code></pre> <p>You can recognize it is the same command we showed before, but we are starting to make it a bit more dynamic, the only changing part are the folders, but they are now hardcoded in the script itself. This could be the use case when a recurring backup is performed.</p> <p>You can now make it executable:</p> <pre><code>chmod +x backup-home\n</code></pre> <p>And test it by just invoking it:</p> <pre><code>./backup-home\n</code></pre>"},{"location":"system-administration/helper-scripts/#3-adding-parameters-for-flexibility","title":"3. Adding Parameters for Flexibility","text":"<p>Now that we have our first version of the helper script, you might want to back up different directories or specify a destination.</p> <p>Let's update the script to introduce dynamic arguments to populate parameters:</p> <pre><code>#!/bin/bash\n# backup.sh - flexible rsync backup helper\n\n# A simple help to show the order of arguments when no arguments are provided.\n\nif [ \"$#\" -lt 2 ]; then\n  echo \"Usage: $0 &lt;source_dir&gt; &lt;destination_dir&gt;\"\n  exit 1\nfi\n\n# Parameter for the source folder\nSOURCE=\"$1\"\n# Parameter for the destination folder\nDEST=\"$2\"\n\nrsync -avh --progress --delete \\\n  --exclude='.cache' \\\n  --exclude='*.tmp' \\\n  \"$SOURCE\" \"$DEST\"\n</code></pre> <p>And now you are ready to test it with any folder:</p> <pre><code>backup.sh /home/user/ /mnt/backup/home_user/\nbackup.sh /var/www/ /mnt/backup/websites/\n</code></pre>"},{"location":"system-administration/helper-scripts/#4-make-it-available-system-wide","title":"4. Make it available system-wide","text":"<p>Now that we have the script in place, we can also make it available system-wide, so we can use it like any other general purpose CLI tools available to all users.</p> <p>To do so, it is enough to move it to the <code>/usr/local/bin</code> folder.</p> <pre><code>sudo mv backup /usr/local/bin\n</code></pre> <p>You will now be able to use it in cron schedules, directly from the CLI or embed it in other automations that you have running in your system.</p>"},{"location":"system-administration/rebooting_practices/","title":"Rebooting a system","text":"<p>To prevent unplanned downtime, it's crucial to take specific precautions before initiating a reboot. The method followed to reboot a system can significantly impact whether the process is successful.</p>"},{"location":"system-administration/rebooting_practices/#practice","title":"Practice","text":"<p>Considerations before rebooting a system.</p> <p>1. Active Users</p> <p>Minimise disruption for logged in users by notifying them about the reboot. - Use the <code>who</code> or <code>w</code> commands to see a list of logged-in users. - Send a message to all logged in users, warning them of the impending reboot.      <pre><code>wall \"This system will be rebooted by the Ops team at 13:00 today\"\n</code></pre></p> <p>2. Open Files</p> <p>Confirm that all important files have been saved and that no processes are writing data to the disk. - Force all data in the buffer to be written to the disk.      <pre><code>sync\n</code></pre></p> <p>3. Mounted filesystems</p> <p>Mount failures at boot time can prevent the system from fully booting up. Check that all mounts can be mounted successfully before rebooting the system.</p> <ul> <li>List all currently mounted filesystems.      <pre><code>mount\n</code></pre></li> <li>Verify that all mounts in <code>/etc/fstab</code> can be mounted successfully.      <pre><code>mount -a\n</code></pre></li> </ul> <p>4. Scheduled jobs</p> <p>Rebooting in the middle of a backup or maintenance job can result in an incomplete state. Check the scheduling of jobs and schedule the reboot accordingly.</p> <ul> <li>List currently scheduled jobs.       <pre><code>crontab -l\n</code></pre></li> </ul>"},{"location":"system-administration/recursive-options/","title":"When using recursive options in a command","text":"<p>Have you ever typed a command and hit enter before you meant to? That happens to us all, and if the command runs recursively in some way, that can have far reaching effects for your system.</p> <p>Classical examples of a difficult-to-fix mistake: <pre><code>chown -R myuser:mygroup /\nrm -rf /home\n</code></pre></p>"},{"location":"system-administration/recursive-options/#practice","title":"Practice","text":"<p>When using an recursing option for a command, add the recursive options in the end, as such:</p> <pre><code># chmod 0700 /the/path/ends/here -R\n# chown myuser:mygroup /the/path/ends/here -R\n# rm /home/myself/garbage -rf\n</code></pre> <p>When entering the recursive option last, that allows you to double check if any paths are correct, before you add the recursive option - and fire off your command. If you accidentally press enter before the full correct path is typed, the effect of the command is not recursive - meaning your mistake causes less damage.</p>"}]}